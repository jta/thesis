\section{Introduction}

Fueled largely by economies of scale in both software management and hardware acquisition and maintenance, the emergence of cloud computing and content distribution networks has lead to a profound shift in Internet traffic.


Whereas the advent of broadband connections init
In contrast to the highly distributed nature of peer-to-peer software which 

For one, never has so much been distributed by so few to so many.
% AS dist graph
% netflix

% Accountability, resilience
Alongside this increased centralization of resources comes a heightened sense of accountability.
The commercial nature of the agreements between customers and cloud hosting companies such as AWS \cite{} often involve the establishment of service level agreements.
Even where such agreements are not explicit, such as the base option of tiered services such as Dropbox or Heroku, there is an inherent need to minimize the impact of outages -- non paying customers may be less demanding, but they are also less sensitive to the switching cost incurred in changing service provider.

Unfortunately, the federated, best-effort nature of the Internet seemingly does not lend itself to such high expectations.
All intervening equipment is subject to failure along a network path, with recovery time depending not only on \emph{what} happened, but also \emph{where}.
Within their networks, operators are free to instrument and deploy arbitrarily complex tools to meet requirements. 
This freedom has lead to a proliferation of proposed improvements in resilience for intradomain settings \cite{}.
Outside their networks however, operators are reduced to negotiating service level agreements with their own providers who are invariably unable to cater for such specialized demands \cite{}.
Even if such terms were met, the Internet architecture provides limited visibility beyond one's own domain.
This opaqueness, in addition to the fact that failures themselves may be distributed in nature \cite{}, severely limits the ability for a customer network to trace remote problems to a single source \cite{}.
With no means to hold providers accountable for failures, there is little to enforce the terms of a truly end-to-end SLA.

The establishment of SLAs for network availability have traditionally been prefered within the telecom industry in particular, and represent one possible approach to ensuring resilience. An alternative approach is offered by the end-to-end principle. 
Since end hosts possess both inherent knowledge of application needs and fine-grained measurements on end-to-end path characteristics, the transport layer is often identified as a more natural fit for dealing with fault tolerance at an Internet scale.
Both SCTP \cite{} and MPTCP \cite{} enable transparent fail-over through multihoming.
In spite of providing significant features, neither is likely to be widely deployed in the near future: the former lacks critical middlebox support, while the latter is still undergoing standardization.
Furthermore, the requirement for end-host multihoming has also posed a barrier to deployment.
Finally, all transport approaches to fail-over confine knowledge of faults within an individual flow: each flow must detect failures independently, even when many flows are affected by the same fault.









% Transport is not the answer

% Can we do better with SDN?


